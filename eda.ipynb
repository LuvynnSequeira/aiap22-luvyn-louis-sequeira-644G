{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA) - Phishing Website Detection\n",
        "\n",
        "**Author:** AIAP Batch 22 Assessment  \n",
        "**Date:** November 2025  \n",
        "**Dataset:** Phishing Website Detection Database\n",
        "\n",
        "## Objective\n",
        "This notebook performs an exploratory data analysis on the phishing website dataset to understand the characteristics of phishing vs legitimate websites and inform feature engineering and model selection for the machine learning pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Loading and Initial Exploration\n",
        "\n",
        "**Purpose:** Load the phishing dataset from SQLite database and perform initial inspection to understand the data structure, size, and basic characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to SQLite database and load data\n",
        "conn = sqlite3.connect('data/phishing.db')\n",
        "df = pd.read_sql_query(\"SELECT * FROM phishing_data\", conn)\n",
        "conn.close()\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of samples: {df.shape[0]}\")\n",
        "print(f\"Number of features: {df.shape[1]}\")\n",
        "\n",
        "# Display first few rows\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The dataset contains 10,500 website samples with 16 features. The 'Unnamed: 0' column is an index, and 'label' is the target variable (0 = legitimate, 1 = phishing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"=\"*50)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\"*50)\n",
        "df.info()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATA TYPES SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Quality Assessment\n",
        "\n",
        "**Purpose:** Check for missing values, duplicates, and data quality issues that may affect model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=\"*50)\n",
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Feature': missing_values.index,\n",
        "    'Missing Count': missing_values.values,\n",
        "    'Percentage': missing_percentage.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "print(missing_df)\n",
        "\n",
        "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"Total cells: {df.shape[0] * df.shape[1]}\")\n",
        "print(f\"Percentage of missing data: {(df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "print(\"=\"*50)\n",
        "print(\"DUPLICATE ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "print(f\"Percentage of duplicates: {(duplicates / len(df) * 100):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conclusion:** The dataset has missing values in the LineOfCode feature which will need to be handled during preprocessing. No duplicate rows were found, which is good for model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Target Variable Analysis\n",
        "\n",
        "**Purpose:** Understand the distribution of the target variable (phishing vs legitimate websites) to assess class balance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable distribution\n",
        "print(\"=\"*50)\n",
        "print(\"TARGET VARIABLE DISTRIBUTION\")\n",
        "print(\"=\"*50)\n",
        "label_counts = df['label'].value_counts()\n",
        "label_percentages = df['label'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(f\"\\nLabel Distribution:\")\n",
        "print(f\"  Legitimate websites (0): {label_counts[0]} ({label_percentages[0]:.2f}%)\")\n",
        "print(f\"  Phishing websites (1): {label_counts[1]} ({label_percentages[1]:.2f}%)\")\n",
        "\n",
        "# Visualize target distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Count plot\n",
        "sns.countplot(data=df, x='label', palette='Set2', ax=axes[0])\n",
        "axes[0].set_title('Target Variable Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Label (0=Legitimate, 1=Phishing)', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].set_xticklabels(['Legitimate (0)', 'Phishing (1)'])\n",
        "\n",
        "# Add counts on bars\n",
        "for container in axes[0].containers:\n",
        "    axes[0].bar_label(container)\n",
        "\n",
        "# Pie chart\n",
        "colors = ['#8dd3c7', '#fb8072']\n",
        "axes[1].pie(label_counts, labels=['Legitimate', 'Phishing'], autopct='%1.1f%%', \n",
        "            colors=colors, startangle=90)\n",
        "axes[1].set_title('Target Variable Percentage', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The dataset shows a class imbalance with approximately 70% phishing websites and 30% legitimate websites. This imbalance will need to be considered when selecting evaluation metrics and potentially using techniques like SMOTE or class weighting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Statistical Summary of Numerical Features\n",
        "\n",
        "**Purpose:** Generate descriptive statistics for all numerical features to understand their distributions, ranges, and central tendencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary of numerical features\n",
        "print(\"=\"*50)\n",
        "print(\"STATISTICAL SUMMARY - NUMERICAL FEATURES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Select numerical columns (excluding index and label)\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numerical_cols = [col for col in numerical_cols if col not in ['Unnamed: 0', 'label']]\n",
        "\n",
        "print(f\"\\nNumerical features: {numerical_cols}\")\n",
        "print(f\"\\nNumber of numerical features: {len(numerical_cols)}\\n\")\n",
        "\n",
        "# Display statistics\n",
        "df[numerical_cols].describe().T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Categorical Features Analysis\n",
        "\n",
        "**Purpose:** Examine categorical features (Industry and HostingProvider) to understand their distributions and unique values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical features analysis\n",
        "categorical_cols = ['Industry', 'HostingProvider']\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Unique values: {df[col].nunique()}\")\n",
        "    print(f\"  Missing values: {df[col].isnull().sum()}\")\n",
        "    print(f\"  Top 10 most common values:\")\n",
        "    print(df[col].value_counts().head(10))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top categories\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Industry distribution\n",
        "industry_top = df['Industry'].value_counts().head(15)\n",
        "sns.barplot(x=industry_top.values, y=industry_top.index, palette='viridis', ax=axes[0])\n",
        "axes[0].set_title('Top 15 Industries', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Count', fontsize=12)\n",
        "axes[0].set_ylabel('Industry', fontsize=12)\n",
        "\n",
        "# HostingProvider distribution\n",
        "hosting_top = df['HostingProvider'].value_counts().head(15)\n",
        "sns.barplot(x=hosting_top.values, y=hosting_top.index, palette='plasma', ax=axes[1])\n",
        "axes[1].set_title('Top 15 Hosting Providers', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Count', fontsize=12)\n",
        "axes[1].set_ylabel('Hosting Provider', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Both Industry and HostingProvider have high cardinality (many unique values). For the ML pipeline, we'll need to apply encoding techniques such as target encoding or frequency encoding to handle these categorical features effectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Feature Distributions Visualization\n",
        "\n",
        "**Purpose:** Visualize the distribution of numerical features using histograms and box plots to identify skewness and potential outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution plots for numerical features\n",
        "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    if idx < len(axes):\n",
        "        df[col].hist(bins=50, ax=axes[idx], edgecolor='black', color='skyblue')\n",
        "        axes[idx].set_title(f'{col} Distribution', fontsize=12, fontweight='bold')\n",
        "        axes[idx].set_xlabel(col, fontsize=10)\n",
        "        axes[idx].set_ylabel('Frequency', fontsize=10)\n",
        "        \n",
        "        # Add mean and median lines\n",
        "        axes[idx].axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[col].mean():.2f}')\n",
        "        axes[idx].axvline(df[col].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df[col].median():.2f}')\n",
        "        axes[idx].legend(fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots to identify outliers\n",
        "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    if idx < len(axes):\n",
        "        sns.boxplot(data=df, y=col, ax=axes[idx], color='lightcoral')\n",
        "        axes[idx].set_title(f'{col} Box Plot', fontsize=12, fontweight='bold')\n",
        "        axes[idx].set_ylabel(col, fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conclusion:** Many features show right-skewed distributions and contain outliers. Features like LineOfCode, LargestLineLength, and NoOfImage have significant outliers. We'll need to consider outlier handling and potentially feature scaling in our preprocessing pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Correlation Analysis\n",
        "\n",
        "**Purpose:** Examine correlations between numerical features and identify multicollinearity that might affect model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix including target variable\n",
        "correlation_features = numerical_cols + ['label']\n",
        "correlation_matrix = df[correlation_features].corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Heatmap - All Features', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print features most correlated with target\n",
        "print(\"=\"*50)\n",
        "print(\"CORRELATION WITH TARGET VARIABLE\")\n",
        "print(\"=\"*50)\n",
        "target_correlation = correlation_matrix['label'].sort_values(ascending=False)\n",
        "print(\"\\nFeatures sorted by correlation with target (label):\")\n",
        "print(target_correlation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify highly correlated feature pairs (potential multicollinearity)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"HIGHLY CORRELATED FEATURE PAIRS\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nFeature pairs with correlation > 0.7:\")\n",
        "\n",
        "correlation_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.7 and correlation_matrix.columns[i] != 'label' and correlation_matrix.columns[j] != 'label':\n",
        "            correlation_pairs.append({\n",
        "                'Feature 1': correlation_matrix.columns[i],\n",
        "                'Feature 2': correlation_matrix.columns[j],\n",
        "                'Correlation': correlation_matrix.iloc[i, j]\n",
        "            })\n",
        "\n",
        "if correlation_pairs:\n",
        "    correlation_df = pd.DataFrame(correlation_pairs).sort_values('Correlation', ascending=False)\n",
        "    print(correlation_df)\n",
        "else:\n",
        "    print(\"No feature pairs with correlation > 0.7 found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The correlation analysis reveals which features have the strongest relationships with the target variable. Features with high correlation (positive or negative) are likely to be important predictors. High correlation between features (multicollinearity) might be problematic for some models like linear regression but less so for tree-based models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Feature Comparison - Phishing vs Legitimate Websites\n",
        "\n",
        "**Purpose:** Compare feature distributions between phishing and legitimate websites to identify distinguishing characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare key features between phishing and legitimate sites\n",
        "key_features = ['DomainAgeMonths', 'NoOfURLRedirect', 'NoOfPopup', 'NoOfiFrame', \n",
        "                'Robots', 'IsResponsive']\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(key_features):\n",
        "    # Box plot comparison\n",
        "    df_plot = df[[feature, 'label']].copy()\n",
        "    df_plot['Website Type'] = df_plot['label'].map({0: 'Legitimate', 1: 'Phishing'})\n",
        "    \n",
        "    sns.boxplot(data=df_plot, x='Website Type', y=feature, \n",
        "                palette={'Legitimate': '#8dd3c7', 'Phishing': '#fb8072'}, ax=axes[idx])\n",
        "    axes[idx].set_title(f'{feature} Comparison', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Website Type', fontsize=10)\n",
        "    axes[idx].set_ylabel(feature, fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical comparison between phishing and legitimate websites\n",
        "print(\"=\"*50)\n",
        "print(\"STATISTICAL COMPARISON BY WEBSITE TYPE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "comparison_stats = df.groupby('label')[numerical_cols].agg(['mean', 'median', 'std'])\n",
        "print(\"\\nMean values by website type:\")\n",
        "print(comparison_stats.xs('mean', axis=1, level=1).T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Key differences observed between phishing and legitimate websites:\n",
        "- **DomainAgeMonths**: Legitimate websites tend to have older domains\n",
        "- **NoOfURLRedirect**: Phishing sites often have more URL redirects\n",
        "- **Robots**: Legitimate sites are more likely to have robots.txt\n",
        "- **IsResponsive**: Legitimate sites tend to be more responsive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Findings and Interpretation\n",
        "\n",
        "Based on the exploratory data analysis, here are the key findings:\n",
        "\n",
        "### 1. **Dataset Characteristics**\n",
        "- 10,500 samples with 16 features\n",
        "- Class imbalance: ~70% phishing, ~30% legitimate websites\n",
        "- Missing values in LineOfCode feature (~3,000 missing values)\n",
        "- High cardinality categorical features (Industry, HostingProvider)\n",
        "\n",
        "### 2. **Feature Quality**\n",
        "- Most numerical features are right-skewed with outliers\n",
        "- Several features show strong correlation with target variable\n",
        "- No duplicate records in the dataset\n",
        "\n",
        "### 3. **Key Discriminating Features**\n",
        "Based on the analysis, the following features appear most useful for classification:\n",
        "- **DomainAgeMonths**: Younger domains are associated with phishing\n",
        "- **NoOfURLRedirect**: Higher redirects indicate phishing\n",
        "- **Robots**: Presence of robots.txt suggests legitimate sites\n",
        "- **IsResponsive**: Responsive design more common in legitimate sites\n",
        "- **NoOfPopup**: Phishing sites may have more popups\n",
        "\n",
        "### 4. **Data Quality Issues**\n",
        "- Missing values in LineOfCode require imputation\n",
        "- Outliers present in multiple features\n",
        "- High cardinality categorical variables need appropriate encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendations for ML Pipeline\n",
        "\n",
        "Based on the EDA findings, the following preprocessing steps and modeling choices are recommended:\n",
        "\n",
        "### Feature Engineering & Preprocessing:\n",
        "1. **Handle Missing Values**: \n",
        "   - Impute LineOfCode missing values using median or KNN imputation\n",
        "   \n",
        "2. **Outlier Treatment**:\n",
        "   - Consider capping outliers or using robust scaling methods\n",
        "   - Tree-based models are naturally robust to outliers\n",
        "   \n",
        "3. **Categorical Encoding**:\n",
        "   - Use target encoding or frequency encoding for Industry and HostingProvider\n",
        "   - Handle high cardinality appropriately\n",
        "   \n",
        "4. **Feature Scaling**:\n",
        "   - Standardization or normalization for models sensitive to scale (Logistic Regression, SVM, Neural Networks)\n",
        "   - Not required for tree-based models\n",
        "   \n",
        "5. **Handle Class Imbalance**:\n",
        "   - Use stratified sampling for train-test split\n",
        "   - Consider class weights in model training\n",
        "   - Alternatively, use SMOTE for oversampling minority class\n",
        "\n",
        "### Model Selection:\n",
        "1. **Primary Models to Consider**:\n",
        "   - **Random Forest**: Handles non-linear relationships, robust to outliers, provides feature importance\n",
        "   - **XGBoost/LightGBM**: High performance, handles missing values, good with imbalanced data\n",
        "   - **Logistic Regression**: Baseline model for interpretability\n",
        "   \n",
        "2. **Ensemble Methods**: Combine multiple models for better performance\n",
        "\n",
        "### Evaluation Metrics:\n",
        "Given class imbalance, use:\n",
        "- **Primary**: F1-score, Precision, Recall\n",
        "- **ROC-AUC**: Overall model discrimination ability\n",
        "- **Confusion Matrix**: Understand false positives vs false negatives\n",
        "- **Avoid**: Accuracy alone (misleading with imbalanced data)\n",
        "\n",
        "### Other Considerations:\n",
        "- Use cross-validation for robust performance estimation\n",
        "- Perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV\n",
        "- Monitor for overfitting with validation curves\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
